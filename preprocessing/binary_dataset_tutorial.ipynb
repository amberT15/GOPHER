{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d1d2d8b",
   "metadata": {},
   "source": [
    "## This notebook goes through dataset creation for binary models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8122b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import subprocess\n",
    "import yaml, os, shutil, sys\n",
    "import json\n",
    "from natsort import natsorted\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9a0bf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_genome(genome): \n",
    "    '''choose genome file paths for chr size, fa and unmappable genome segments (optional)'''\n",
    "    genome_dict = {'hg38': {'size':  '/home/amber/ref/hg38/hg38.chrom.sizes',\n",
    "                              'fa':  '/home/amber/ref/hg38/hg38.fa',\n",
    "                              'unmap':  '/home/amber/ref/hg38/hg38_unmap.bed'}}\n",
    "            \n",
    "    assert genome in genome_dict.keys(), 'Unknown genome!'\n",
    "    return genome_dict[genome]\n",
    "    \n",
    "def write_basset_samplefile(bed_filepaths, basset_samplefile):\n",
    "    print('Generating merged samplefile for the entire bedfile set')\n",
    "    df = pd.DataFrame(columns =['identifier', 'file'])\n",
    "    # per file, get the filename\n",
    "    for b, bedfile_path in enumerate(bed_filepaths):\n",
    "        # make entry in basenji samplefile\n",
    "        df.loc[b] = [os.path.basename(bedfile_path).split('.b')[0], bedfile_path]\n",
    "    # write to csv files\n",
    "    df.to_csv(basset_samplefile, index=None, header=None, sep='\\t')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f357259a",
   "metadata": {},
   "source": [
    "\n",
    "The most relevant options here are:\n",
    "\n",
    "| Argument | Note |\n",
    "|:---|:---|\n",
    "| chroms_only | if 'all' creates train, val, test, if specific chromosomes then creates test set from only those|\n",
    "| input_size| input size of the genomic|\n",
    "| base_dir | the output directory for the tfr files |\n",
    "| bigwig_paths_pattern | regexp pattern that will collect all the bigwig files |\n",
    "| bigwig_filepaths | can set this instead as a list of all the bws if don't want to use glob |\n",
    "| bedfile_paths_pattern | same for bed files of genomic regions to focus on IF you want peak centered dataset |\n",
    "| bed_filepaths | --.-- |\n",
    "| pool_window | bin size, if set to 1 can bin later in the training |\n",
    "| dilation_rate | fraction of data to include, can set to 0.1 to test pipelines|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4380510d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 2048\n",
    "base_dir = '../data/tfr_datasets/'\n",
    "bedfile_paths_pattern = '../data/*/*peaks.bed'\n",
    "bed_filepaths = [f for f in glob.glob(bedfile_paths_pattern) if f.endswith('bed') or f.endswith('gz')]\n",
    "valid_chr = 'chr9'\n",
    "test_chr = 'chr8'\n",
    "genome = 'hg38'\n",
    "dilation_rate = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d128a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating merged samplefile for the entire bedfile set\n"
     ]
    }
   ],
   "source": [
    "basset_samplefile = os.path.join(base_dir, 'basset_samplefile.csv')\n",
    "write_basset_samplefile(bed_filepaths, basset_samplefile)# write pre-requisite file for the pipeline specifying bed paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b086139e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = os.path.join(base_dir, 'config.yaml')\n",
    "config = {}\n",
    "\n",
    "config['genomefile'] = set_genome(genome)\n",
    "\n",
    "config['chroms']={'valid': valid_chr, 'test': test_chr}\n",
    "config['input'] = {'downsample': dilation_rate, 'size':input_size}\n",
    "config['samplefile'] = {'basset': basset_samplefile}\n",
    "\n",
    "config['output'] = {'dir': base_dir, \n",
    "                   'prefix': 'i_%i_binary' % (config['input']['size'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4208021e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml', 'w') as file:\n",
    "    documents = yaml.dump(config, file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a97a9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak centering\n",
      "Generating bed region combined file for all TFs\n",
      "Ignoring chrY +\n",
      "Warning: the index file is older than the FASTA file.\n",
      "Warning: the index file is older than the FASTA file.\n",
      "Warning: the index file is older than the FASTA file.\n",
      "LOADING DATA\n",
      "LOADING DATA\n",
      "LOADING DATA\n"
     ]
    }
   ],
   "source": [
    "! ./bed_to_tfr.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f900833e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
